icl_tasks:
#### Removing because slow and likely too hard (0 shot and 10 shot have same results, although around 3.5% somehow)
#-
#  label: hellaswag_zeroshot
#  dataset_uri: language_understanding/hellaswag.jsonl
#  num_fewshot: [0]
#  icl_task_type: multiple_choice
#### Removing because 0%
#-
#  label: jeopardy
#  dataset_uri: world_knowledge/jeopardy_all.jsonl
#  num_fewshot: [10]
#  icl_task_type: language_modeling
#  continuation_delimiter: "\nAnswer: "
#  has_categories: true
-
  label: bigbench_qa_wikidata
  dataset_uri: world_knowledge/bigbench_qa_wikidata.jsonl
  num_fewshot: [10]
  icl_task_type: language_modeling
-
  label: arc_easy
  dataset_uri: world_knowledge/arc_easy.jsonl
  num_fewshot: [10]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
#### Removed because useless for tiny models
#-
#  label: arc_challenge
#  dataset_uri: world_knowledge/arc_challenge.jsonl
#  num_fewshot: [10]
#  icl_task_type: multiple_choice
#  continuation_delimiter: "\nAnswer: "
#### Removed because -8% somehow, much worse than random chance. Suspect it's a bad test?
#-
#  label: copa
#  dataset_uri: commonsense_reasoning/copa.jsonl
#  num_fewshot: [0]
#  icl_task_type: multiple_choice
-
  label: commonsense_qa
  dataset_uri: commonsense_reasoning/commonsense_qa.jsonl
  num_fewshot: [10]
  icl_task_type: multiple_choice
-
  label: piqa
  dataset_uri: commonsense_reasoning/piqa.jsonl
  num_fewshot: [10]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
-
  label: openbook_qa
  dataset_uri: commonsense_reasoning/openbook_qa.jsonl
  num_fewshot: [0]
  icl_task_type: multiple_choice
-
  label: lambada_openai
  dataset_uri: language_understanding/lambada_openai.jsonl
  num_fewshot: [0]
  icl_task_type: language_modeling
#### Removing because slow and likely too hard (0 shot and 10 shot have same results, although around 3.5% somehow)
#-
#  label: hellaswag
#  dataset_uri: language_understanding/hellaswag.jsonl
#  num_fewshot: [10]
#  icl_task_type: multiple_choice
-
  label: winograd
  dataset_uri: language_understanding/winograd_wsc.jsonl
  num_fewshot: [0]
  icl_task_type: schema
-
  label: winogrande
  dataset_uri: language_understanding/winogrande.jsonl
  num_fewshot: [0]
  icl_task_type: schema
#### Removing because easy to game
#-
#  label: bigbench_dyck_languages
#  dataset_uri: symbolic_problem_solving/bigbench_dyck_languages.jsonl
#  num_fewshot: [10]
#  icl_task_type: language_modeling
-
  label: agi_eval_lsat_ar
  dataset_uri: symbolic_problem_solving/agi_eval_lsat_ar.jsonl
  num_fewshot: [3]
  icl_task_type: multiple_choice
-
  label: bigbench_operators
  dataset_uri: symbolic_problem_solving/bigbench_operators.jsonl
  num_fewshot: [10]
  icl_task_type: language_modeling
#### removing because 0% accuracy
#-
#  label: bigbench_repeat_copy_logic
#  dataset_uri: symbolic_problem_solving/bigbench_repeat_copy_logic.jsonl
#  num_fewshot: [10]
#  icl_task_type: language_modeling
#### removing because slow and 0.24% accuracy
#-
#  label: squad
#  dataset_uri: reading_comprehension/squad.jsonl
#  num_fewshot: [10]
#  icl_task_type: language_modeling
-
  label: coqa
  dataset_uri: reading_comprehension/coqa.jsonl
  num_fewshot: [0]
  icl_task_type: language_modeling
-
  label: boolq
  dataset_uri: reading_comprehension/boolq.jsonl
  num_fewshot: [10]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
#### Removing because slow and 0.5% accuracy
#-
#  label: bigbench_language_identification
#  dataset_uri: language_understanding/bigbench_language_identification.jsonl
#  num_fewshot: [10]
#  icl_task_type: multiple_choice
