
---------- TICKET 040
Title    : add conviction support to base_complete
Status   : PENDING
    In the future, we will be creating a full tool around this, but in the meantime, we will be implementing logic in base_complete to show the conviction rate per token. It'll be triggered by a new terminator, "@@@". When "###" is used, the current logic will work as-is. When "@@@" is used, then this will happen:

    1. It won't start with the user-completed prompt in dark gray. Instead, it will stream one token per line, using the following template:
tok     | 0.1234
    So, as you can see, it will be the token, padded by 8 spaces, then a delimiting pipe, then the conviction formatted like 0.0000 .
    1 special thing to note: if the token has any whitespace characters, they will get replaced by a period, but printed in dark gray.
----------

---------- TICKET 037
Title    : wandb log last_hidden_layer average size (vector length)
Status   : PENDING
----------

---------- TICKET 034
Title    : Cleanup wandb
Status   : PENDING
    Currently, the logging of the benchmarks mid-run go into the same place as when the run is completed. These should get separated. Maybe like "train_benchmarks" and "final_benchmarks".
----------

---------- TICKET 039
Title    : wandb move some logging values to "debug"
Status   : PENDING
    conviction_loss, ce_loss, last_hidden_layer average size
----------

---------- TICKET 036
Title    : Add parameter to allow conviction to train lm_head
Status   : PENDING
----------

---------- TICKET 038
Title    : investigate gradient norms
Status   : PENDING
    output_projection and conviction gradient norms get larger and larger over the course of a run. Why is this, and is there something that can be done to reduce this?
    Do output_projection grad norms explode like that when conviction is disabled?
----------

---------- TICKET 011
Title    : Create corpus size calculator
Status   : PENDING
    Calculates corpus size in tokens. Creates .meta.json file for each parquet file which caches the token size for quick lookup. The meta file captures the timestamp of the parquet file, and if they don't match, then the meta file is recalculated.
---------- TICKET 010
Title    : Create corpus mixer
Status   : PENDING
    Script which takes corpora and mixes them by a given ratio.
    Depends on [011]
---------- TICKET 033
Title    : Create "tinystories/fineweb-edu" corpus
Status   : PENDING
    Mix corpora together, probably 50:50
    Depends on [012]
----------

---------- TICKET 030
Title    : Gating
Status   : ICEBOX
    Add another parameter, "gated", which when true, adds a separate up_proj matrix which gets element-wise multiplied before going through the down_proj matrix. Maybe it should be named to "gate_signal"
    Note: simply optimiziation; low priority
----------

---------- TICKET 031
Title    : Biases
Status   : ICEBOX
    Add another parameter, "enable_bias" or something like that, which enables biases for basically all matrices (unless there are any that don't make sense?)
    Note: simply (potential) optimiziation; low priority
----------

---------- TICKET 018
Title    : Calculate optimal run sizes
Status   : ICEBOX
    Once wandb logging is fixed, it'll be trivial to determine optimal run sizes as long as we do several different runs.
----------

---------- TICKET 020
Title    : Calculate optimal hyperparameters
Status   : ICEBOX
----------

---------- TICKET 013
Title    : Conviction head completion tool
Status   : PENDING
    Since conviction is separate from the text itself, we need a way to run completions and see their conviction. Maybe hack mikupad to show conviction per token? Probably new simple web tool.
----------

---------- TICKET 041
Title    : 
Status   : PENDING

----------
